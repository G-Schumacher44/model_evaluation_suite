{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa38e150",
   "metadata": {},
   "source": [
    "> üí° **Recommended Environment**:  \n",
    "> Run this notebook in the `model_eval_suite` Conda environment for best results.  \n",
    "> See setup instructions in the [Usage Guide](../resource_hub/usage_guide.md).\n",
    ">\n",
    "> ‚ö†Ô∏è If you're running this outside Conda, you can install dependencies manually:\n",
    "> Uncomment the line below to install from the root requirements file.\n",
    "> ```python\n",
    "> # !pip install -r ../../requirements.txt\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db339746",
   "metadata": {},
   "source": [
    "# üîß Quick Start: Model Evaluation Suite\n",
    "This notebook provides a minimal working example of running a complete model evaluation using the suite.\n",
    "\n",
    "It assumes that:\n",
    "- You have already prepared your data (train/test/holdout)\n",
    "- You have edited the YAML configuration files under `config/`\n",
    "- You want a fast, no-frills path to getting metrics, plots, and a dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf1b6b",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Set Config Paths\n",
    "Define the paths to your YAML configuration files for:\n",
    "- Data prep (optional)\n",
    "- Model evaluation\n",
    "- Champion validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = 'config/data_prep_config.yaml'\n",
    "eval_config = 'config/classifier/logreg.yaml'  # Swap for your model\n",
    "validation_config = 'config/xgb_validation.yaml'  # Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d01b2b",
   "metadata": {},
   "source": [
    "## üßº Step 2: (Optional) Prepare Data\n",
    "Skip this step if you've already created `train.csv`, `test.csv`, and `holdout.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_eval_suite import prep_data\n",
    "prep_data(user_config_path=data_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47102d",
   "metadata": {},
   "source": [
    "## üìä Step 3: Run a Model Experiment\n",
    "Train and evaluate your model using the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf883723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_eval_suite import run_experiment\n",
    "run_experiment(user_config_path=eval_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194e10b",
   "metadata": {},
   "source": [
    "## üèÜ Step 4: Validate the Champion Model (Optional)\n",
    "Evaluate a registered MLflow model against the holdout set, optionally comparing to a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b82dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_eval_suite import validate_champion\n",
    "validate_champion(config_path=validation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588aa4a",
   "metadata": {},
   "source": [
    "## ‚úÖ Done!\n",
    "Artifacts will be saved to the paths specified in your YAML:\n",
    "- `data/reports/` for dashboards and metrics\n",
    "- `data/plots/` for visualizations\n",
    "- MLflow for model tracking and experiment logging\n",
    "\n",
    "To customize behavior, modify the relevant YAMLs under `config/` or `config_resources/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
