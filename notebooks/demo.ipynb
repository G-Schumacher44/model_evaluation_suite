{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7bf26c6b",
            "metadata": {},
            "source": [
                "> üí° **Recommended Environment**:  \n",
                "> Run this notebook in the `model_eval_suite` Conda environment for best results.  \n",
                "> See setup instructions in the [Usage Guide](../resource_hub/usage_guide.md).\n",
                ">\n",
                "> ‚ö†Ô∏è If you're running this outside Conda, you can install dependencies manually:\n",
                "> Uncomment the line below to install from the root requirements file.\n",
                "> ```python\n",
                "> # !pip install -r ../../requirements.txt\n",
                "> ```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad1229dd",
            "metadata": {},
            "source": [
                "# üß™ Model Evaluation Suite Demo Notebook\n",
                "\n",
                "This notebook demonstrates how to use the **Model Evaluation Suite** to:\n",
                "\n",
                "- Prepare and validate input data.\n",
                "- Run a full modeling pipeline using YAML configuration files.\n",
                "- Log models and artifacts with MLflow.\n",
                "- Evaluate a production candidate against a holdout dataset.\n",
                "- Optionally compare against a **baseline model** for performance drift or uplift.\n",
                "\n",
                "<details><summary>üì¶ Project Structure</summary>\n",
                "\n",
                "This notebook expects the following directories and files to exist:\n",
                "- `config/`: contains user-defined YAML configuration files.\n",
                "- `data/holdout_data/`: contains the holdout CSV used in validation.\n",
                "- `mlruns/`: MLflow tracking output.\n",
                "\n",
                "</details>\n",
                "\n",
                "\n",
                "<details><summary>‚öôÔ∏è Workflow Overview</summary>\n",
                "\n",
                "1. **Prep Data (Optional)** ‚Äì If needed, run data prep to split/train/test and cache sets.\n",
                "2. **Run Experiment** ‚Äì Train model(s) as defined in the YAML file using `run_pipeline`.\n",
                "3. **Validate Champion** ‚Äì Evaluate the registered MLflow model using `validate_and_display`.\n",
                "\n",
                ">YAML-driven configuration allows for full modularity, reproducibility, and MLflow registry integration.\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2672602b",
            "metadata": {},
            "source": [
                "### üìú Configuration Setup\n",
                "\n",
                "This notebook is driven by modular **YAML configuration files**, which serve as the central control system for the evaluation suite.\n",
                "\n",
                "<details><summary>click here to expand section</summary>\n",
                "\n",
                "These YAMLs are edited **upfront** to define the behavior of each stage in the pipeline. See `config_resources/` in the repository for further guidance.\n",
                "\n",
                "The YAML configuration governs:\n",
                "\n",
                "- Filepaths for all inputs and outputs (train/test/holdout, plots, reports, logs)\n",
                "- Model architecture, hyperparameters, and estimator type\n",
                "- Preprocessing and feature engineering behavior\n",
                "- Optional diagnostics modules (e.g., VIF, SHAP, permutation importance)\n",
                "- MLflow tracking settings (URI, run tags, experiment names)\n",
                "- Model type and parameters\n",
                "- Plotting controls and dashboard rendering options\n",
                "- Evaluation behavior: segmentation columns, scoring metrics, baseline model comparison\n",
                "\n",
                "> Prebuilt templates are provided. You can download them at `config_resources/config.zip` and use them as-is or customize them to match your workflow\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6add55f2",
            "metadata": {},
            "source": [
                "### üîß Custom Feature Engineering\n",
                "\n",
                "This suite supports plug-and-play custom transformers via the `feature_engineering` block in your YAML. \n",
                "\n",
                "<details><summary>click here to expand</summary>\n",
                "\n",
                "Your transformer should follow scikit-learn's `fit/transform` API and be referenced like this:\n",
                "\n",
                "```yaml\n",
                "feature_engineering:\n",
                "  run: true\n",
                "  module: \"my_project.custom_features\"\n",
                "  class_name: \"MyFeatureTransformer\"\n",
                "```\n",
                "\n",
                ">Your transformer must follow the fit/transform interface. See `docs/feature_engineering.md` for a full example. \n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "825ba83a",
            "metadata": {},
            "source": [
                "## üìö Dashboard Guidence\n",
                "\n",
                "#### üìâ Pre-Model Diagnostics Dashboard\n",
                "\n",
                "This optional module runs before any model training or validation occurs. It provides key insights into the integrity and statistical structure of your input data. It is driven by the `pre_model_diagnostics` block in your YAML and is best used in notebook workflows.\n",
                "\n",
                "<details><summary>click here to expand section</summary>\n",
                "\n",
                "- **Overview**  \n",
                "  Summary of the input dataset, schema, and basic shape metadata.\n",
                "\n",
                "- **Missingness**  \n",
                "  Tabulates and visualizes missing values by column, with percent missing and optional flag encoding hints.\n",
                "\n",
                "- **Collinearity**  \n",
                "  Includes:\n",
                "  - Pearson correlation heatmap\n",
                "  - Variance Inflation Factor (VIF) plot to detect multicollinearity risks\n",
                "\n",
                "- **Distribution Quality**  \n",
                "  Visualizes skewness and potential distribution anomalies:\n",
                "  - Target column distribution\n",
                "  - Numerical feature histograms\n",
                "  - Outlier detection using IQR boxplots\n",
                "\n",
                "- **Evaluation Plots (via PlotViewer)**  \n",
                "  This tab includes all advanced diagnostics plots:\n",
                "  - VIF plot\n",
                "  - Pearson heatmap\n",
                "  - Outlier boxplots\n",
                "  - Feature-wise skew distributions\n",
                "\n",
                ">These diagnostics are critical for spotting leakage, encoding flaws, and redundancy before any modeling occurs.\n",
                "\n",
                "</details>\n",
                "\n",
                "\n",
                "#### üìä Model Evaluation Dashboard \n",
                "\n",
                "This dashboard provides an interactive summary of the model trained in the experimental run. It visualizes performance on the test set and includes explainability tools to support model diagnostics and stakeholder communication.\n",
                "\n",
                "<details><summary>Summary</summary>\n",
                "\n",
                "- High-level performance metrics (e.g., R¬≤, MAE for regression or Accuracy, F1 for classification)\n",
                "- If **cross-validation** is enabled, a **boxplot of fold-level scores** is included\n",
                "- If a **baseline model** is configured, delta scores are annotated beside the champion‚Äôs metrics\n",
                "\n",
                "</details>\n",
                "\n",
                "<details><summaryBaseline (if applied)</summary>\n",
                "\n",
                "- Displays the same metrics for the baseline model\n",
                "- Highlights any drop or improvement when compared to the current champion\n",
                "\n",
                "</details>\n",
                "\n",
                "<details><summary>Importance</summary>\n",
                "\n",
                "- Feature importance scores from:\n",
                "  - SHAP bar charts (if SHAP enabled)\n",
                "  - Coefficients (for linear models)\n",
                "  - Permutation importance (if enabled)\n",
                "- Useful for debugging and stakeholder reporting\n",
                "\n",
                "</details>\n",
                "\n",
                "<details><summary>Explainability</summary>\n",
                "\n",
                "- SHAP Impact Summary Plot for understanding global feature effects\n",
                "- Optional if SHAP is disabled in your config\n",
                "\n",
                "</details>\n",
                "\n",
                "<details><summary>Plotviewers</summary>\n",
                "\n",
                "**Model Performance Plots**\n",
                "- Interactive evaluation visuals via the plot viewer widget:\n",
                "  - ROC & PR curves (classification)\n",
                "  - Residuals, prediction vs. truth (regression)\n",
                "  - Confusion matrix, threshold plots, calibration, etc.\n",
                "\n",
                "**Distribution Plots**\n",
                "- Always included\n",
                "- Shows feature distributions in the holdout set\n",
                "- Supports quick detection of skew, class imbalance, or feature leakage\n",
                "\n",
                "</details>\n",
                "\n",
                "<details><summary>Metadata</summary>\n",
                "\n",
                "- Full configuration summary:\n",
                "  - YAML config snapshot\n",
                "  - Model and version from MLflow\n",
                "  - Holdout dataset used\n",
                "  - Run ID and export paths\n",
                "\n",
                "</details>\n",
                "\n",
                "<details><summary> Alerts</summary>\n",
                "\n",
                "- Automated audit system that surfaces:\n",
                "  - Warning thresholds (e.g., F1 below expected)\n",
                "  - Cross-validation variance anomalies\n",
                "  - Drift against baseline scores\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ead9dff",
            "metadata": {},
            "source": [
                "#### üìå Core Imports\n",
                "\n",
                "You can access the main runners directly from the package thanks to a clean interface exposed via `__init__.py`. These entrypoints allow you to run each stage of the pipeline from a single import."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37ebf026",
            "metadata": {},
            "outputs": [],
            "source": [
                "from model_eval_suite import run_experiment, validate_champion, prep_data"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dd5d7c7b",
            "metadata": {},
            "source": [
                "#### üì§ Data Preparation\n",
                "\n",
                "If you're starting from raw CSVs, you can use the suite's built-in preprocessing tool, `data_prep.py` to split the data into training, testing, and holdout sets. \n",
                "\n",
                "Skip this if you've already created your `train.csv`, `test.csv`, and `holdout.csv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8d5119f",
            "metadata": {},
            "outputs": [],
            "source": [
                "prep_data(config_path=\"config/data_prep.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ebe4825",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Model Experiment Runs (Demo)\n",
                "\n",
                "This demo walks through multiple model runs using the `salifort_50k` dataset. Although the dataset is optimized for **classification tasks**, it is used for both classification and regression pipelines to demonstrate flexibility and YAML-driven control.\n",
                "\n",
                "We run the following models using the evaluation suite:\n",
                "\n",
                "### üîç Classifier Models\n",
                "\n",
                "- **Gaussian Naive Bayes**  \n",
                "  Config: [config/classifier/guas_nb.yaml](config/classifier/guas_nb.yaml)\n",
                "  \n",
                "- **Logistic Regression**  \n",
                "  Config: [config/classifier/logreg.yaml](config/classifier/logreg.yaml)\n",
                "\n",
                "- **XGBoost Classifier**  \n",
                "  Config: [config/classifier/xgboost.yaml](config/classifier/xgboost.yaml)\n",
                "\n",
                "### üìà Regressor Models\n",
                "\n",
                "- **Linear Regression**  \n",
                "  Config: [config/regressor/linreg.yaml](config/regressor/linreg.yaml)\n",
                "\n",
                "- **XGBoost Regressor**  \n",
                "  Config: [config/regressor/xgboost_reg.yaml](config/regressor/xgboost_reg.yaml)\n",
                "\n",
                "Each model triggers:\n",
                "\n",
                "- An optional **Pre-Model Diagnostics Dashboard** (if enabled in YAML)\n",
                "- A complete **Evaluation Dashboard** with explainability, distributions, and exportable artifacts\n",
                "\n",
                "At the end of the demo, we use the **champion validation** system to validate and crown the two XGBoost models ‚Äî one for classification, one for regression.\n",
                "\n",
                "> All behavior is controlled by the YAML configs. See the `config/` directory or `config_resources/config.zip` for template downloads."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59f2c44b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Naive - Bayes ==========\n",
                "run_experiment(user_config_path=\"config/classifier/guas_nb.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "961404f6",
            "metadata": {},
            "source": [
                "#### ‚ö†Ô∏è SHAP Error Handling\n",
                "\n",
                "<details><summary>click to expand section</summary>\n",
                "\n",
                "Some models ‚Äî such as `SVC`, `SVR`, or `GaussianNB` ‚Äî do not expose traditional feature importance attributes or are incompatible with SHAP explainability tools.\n",
                "\n",
                "This suite handles such situations **gracefully**:\n",
                "\n",
                "- The SHAP tab will be skipped silently if no compatible features are found.\n",
                "- A warning will be logged (but not treated as a failure).\n",
                "- All other evaluation plots and metrics will still render normally.\n",
                "\n",
                "This ensures that the workflow remains robust even for models with limited explainability tooling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "60ddaa40",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Logistic Regression ==========\n",
                "run_experiment(user_config_path=\"config/classifier/logreg.yaml\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "99ae8b8e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== XGBoost Classifier ==========\n",
                "\n",
                "run_experiment(user_config_path=\"config/classifier/xgboost.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "72a96b2e",
            "metadata": {},
            "source": [
                "### üîÅ Cross-Validation Insight\n",
                "\n",
                "<details><summary>click here to expand</summary>\n",
                "\n",
                "If hyperparameter tuning via cross-validation is enabled in the config (`hyperparameter_tuning.run: true`), the dashboard will include an additional boxplot in the **Summary** tab. \n",
                "\n",
                "This plot visualizes the distribution of CV scores across folds for the best-performing parameter set, offering a quick diagnostic of stability and performance variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d9462ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Linear Regression ==========\n",
                "run_experiment(user_config_path=\"config/regressor/linreg.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "01ec242f",
            "metadata": {},
            "source": [
                "#### üö® Automated Alert Auditing\n",
                "<details>\n",
                "<summary>click here to expand section</summary>\n",
                "\n",
                "The validation dashboard includes an **Alerts** tab that surfaces automated audit checks on your model's performance.\n",
                "\n",
                "These alerts are designed to flag potential concerns such as:\n",
                "\n",
                "- Very low precision or recall\n",
                "- High class imbalance\n",
                "- Overfitting indicators (e.g., large delta between train/test scores)\n",
                "- Underwhelming performance against a baseline (if provided)\n",
                "\n",
                "This system provides a lightweight, interpretable review of model quality without requiring custom code or manual thresholding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4ec6860",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== XGBoost Regressor ==========\n",
                "run_experiment(user_config_path=\"config/regressor/xgboost_reg.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "90cb7f4e",
            "metadata": {},
            "source": [
                "## üèÜ Champion Model Validation\n",
                "\n",
                "This section evaluates a registered MLflow model (your *champion*) against a holdout dataset using a dedicated validation YAML configuration.\n",
                "\n",
                "### Key Features\n",
                "\n",
                "- Uses its own standalone YAML file (separate from training experiments)\n",
                "- Accepts an optional **baseline model** for drift detection or performance benchmarking\n",
                "- Automatically generates:\n",
                "  - Confidence interval plots (if applicable)\n",
                "  - Baseline comparison deltas (if a baseline model is provided)\n",
                "  - Alert audits for performance degradation or instability\n",
                "- Produces a complete interactive dashboard with:\n",
                "  - Summary metrics and cross-validation visualizations\n",
                "  - Explainability and feature importance plots\n",
                "  - Distribution visualizations for target and predictions\n",
                "  - Full configuration and environment metadata\n",
                "- **Tags** the evaluated model in the MLflow Registry using your specified `production_tag`\n",
                "\n",
                "üìç This workflow is ideal for pre-deployment validation, regression testing, and model promotion decisions.\n",
                "\n",
                "#### Validation Configurations Used in This Demo\n",
                "\n",
                "- [config/xgb_validation.yaml](config/xgb_validation.yaml) ‚Äì XGBoost classifier\n",
                "- [config/xgb_reg_validation.yaml](config/xgb_reg_validation.yaml) ‚Äì XGBoost regressor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d47619ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Validate Classifer Champion Model ==========\n",
                "validate_champion(config_path=\"config/xgb_validation.yaml\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53eeb200",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========== Validate Regressor Champion Model ==========\n",
                "validate_champion(config_path=\"config/xgb_reg_validation.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f8400519",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ Wrap-Up and Next Steps\n",
                "\n",
                "You‚Äôve now run multiple models through the full suite ‚Äî from preprocessing and diagnostics to evaluation and champion validation.\n",
                "\n",
                "This notebook demonstrates the flexibility of the system, including:\n",
                "\n",
                "- YAML-driven configuration at every stage\n",
                "- Reusable pipelines for both classification and regression tasks\n",
                "- Support for custom feature engineering and hyperparameter tuning\n",
                "- Interactive dashboards for diagnostics and final reporting\n",
                "- MLflow integration for model tracking and registry updates\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- **Test additional models** by duplicating a config YAML.\n",
                "- **Customize features** using your own transformers or fe_config modules.\n",
                "- **Enable advanced diagnostics**, SHAP, and permutation importance as needed.\n",
                "- **Package and deploy** validated models via the MLflow registry.\n",
                "\n",
                "For more examples and config templates, explore the [`config_resources/`](config_resources/) folder or the full [README.md](../README.md).\n",
                "\n",
                "> Questions or suggestions? Feel free to submit an issue or feature request in the repository."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "model_eval_suite_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
